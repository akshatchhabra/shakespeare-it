{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4622,"status":"ok","timestamp":1652392614174,"user":{"displayName":"Ala' Hashesh","userId":"02371999640038913792"},"user_tz":240},"id":"Gh2hARImXhen","outputId":"f5882ef6-5637-43e7-9e21-8687cb89b3d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/.shortcut-targets-by-id/1s39Gy1mP7wbq26JnrSjY-Rsch9COaNmZ/Spring 2022/COMPSCI685/CS685 Project/Sanity Check\n"]}],"source":["# Please note that this code just follows the provided video\n","# Mount data from drive\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n","\n","# Folder name\n","folderName = 'UMass/Spring 2022/COMPSCI685/CS685 Project/Sanity Check'\n","assert folderName is not None, \"[Error] Please enter folder name.\"\n","\n","\n","# Load python files from our folder\n","import sys\n","sys.path.append('/content/drive/My Drive/{}'.format(folderName))\n","\n","%cd /content/drive/My\\ Drive/$folderName/"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"ljwGeYxHZMPm","executionInfo":{"status":"ok","timestamp":1652392614712,"user_tz":240,"elapsed":542,"user":{"displayName":"Ala' Hashesh","userId":"02371999640038913792"}}},"outputs":[],"source":["# Importing libraries\n","import os\n","import numpy as np\n","import pandas as pd\n","\n","def preprocess_sentence(sentence):\n","  sentence = '<start> ' + sentence + ' <end>'\n","  return sentence\n","\n","path = \"data/data.csv\"\n","df = pd.read_csv(path)\n","df[\"English\"] = '<start> ' + df[\"English\"] + ' <end>'\n","df[\"Shakespearean\"] = '<start> ' + df[\"Shakespearean\"] + ' <end>'\n","\n","english = df[\"English\"].tolist()\n","shakespearean = df[\"Shakespearean\"].tolist()"]},{"cell_type":"code","source":["import tensorflow as tf\n","\n","# Convert sequences to tokenizers\n","def tokenize(lang):\n","  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n","      filters='')\n","  \n","  # Convert sequences into internal vocab\n","  lang_tokenizer.fit_on_texts(lang)\n","\n","  # Convert internal vocab to numbers\n","  tensor = lang_tokenizer.texts_to_sequences(lang)\n","\n","  # Pad the tensors to assign equal length to all the sequences\n","  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n","                                                         padding='post')\n","\n","  return tensor, lang_tokenizer"],"metadata":{"id":"8ZkYjMvRxitK","executionInfo":{"status":"ok","timestamp":1652392617308,"user_tz":240,"elapsed":2597,"user":{"displayName":"Ala' Hashesh","userId":"02371999640038913792"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Tokenize the sequences\n","input_tensor, inp_lang = tokenize(english)\n","target_tensor, targ_lang = tokenize(shakespearean)\n","max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"],"metadata":{"id":"XniNESa1xm5_","executionInfo":{"status":"ok","timestamp":1652392617878,"user_tz":240,"elapsed":572,"user":{"displayName":"Ala' Hashesh","userId":"02371999640038913792"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","\n","# Create training and validation sets using an 80/20 split\n","input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n","\n","print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xImcHpYyyA1f","executionInfo":{"status":"ok","timestamp":1652392617879,"user_tz":240,"elapsed":13,"user":{"displayName":"Ala' Hashesh","userId":"02371999640038913792"}},"outputId":"a78c3efe-87be-4ebb-fa0f-c596213af8f1"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["14716 14716 3679 3679\n"]}]},{"cell_type":"code","source":["# Show the mapping b/w word index and language tokenizer\n","def convert(lang, tensor):\n","  for t in tensor:\n","    if t != 0:\n","      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n","      \n","print (\"Input Language; index to word mapping\")\n","convert(inp_lang, input_tensor_train[0])\n","print ()\n","print (\"Target Language; index to word mapping\")\n","convert(targ_lang, target_tensor_train[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LRue-K46yM7t","executionInfo":{"status":"ok","timestamp":1652392617879,"user_tz":240,"elapsed":12,"user":{"displayName":"Ala' Hashesh","userId":"02371999640038913792"}},"outputId":"507233a2-040c-46e7-ddcb-7b637103e26d"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Input Language; index to word mapping\n","1 ----> <start>\n","29 ----> if\n","74 ----> you're\n","27 ----> as\n","220 ----> poor\n","11 ----> a\n","1246 ----> subject\n","27 ----> as\n","24 ----> he\n","19 ----> is\n","11 ----> a\n","101 ----> king\n","3 ----> ,\n","5 ----> you\n","894 ----> definitely\n","32 ----> are\n","220 ----> poor\n","4 ----> .\n","2 ----> <end>\n","\n","Target Language; index to word mapping\n","1 ----> <start>\n","41 ----> if\n","30 ----> thou\n","2331 ----> beest\n","35 ----> as\n","169 ----> poor\n","21 ----> for\n","13 ----> a\n","887 ----> subject\n","35 ----> as\n","304 ----> he's\n","21 ----> for\n","13 ----> a\n","119 ----> king\n","3 ----> ,\n","3974 ----> thou'rt\n","169 ----> poor\n","278 ----> enough\n","4 ----> .\n","2 ----> <end>\n"]}]},{"cell_type":"code","source":["# Essential model parameters\n","BUFFER_SIZE = len(input_tensor_train)\n","BATCH_SIZE = 32\n","steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n","embedding_dim = 256\n","units = 1024\n","vocab_inp_size = len(inp_lang.word_index) + 1\n","vocab_tar_size = len(targ_lang.word_index) + 1"],"metadata":{"id":"DMRqe8WayQBN","executionInfo":{"status":"ok","timestamp":1652392617880,"user_tz":240,"elapsed":10,"user":{"displayName":"Ala' Hashesh","userId":"02371999640038913792"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"],"metadata":{"id":"55U3hPAtySyG","executionInfo":{"status":"ok","timestamp":1652392619792,"user_tz":240,"elapsed":7,"user":{"displayName":"Ala' Hashesh","userId":"02371999640038913792"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["example_input_batch, example_target_batch = next(iter(dataset))\n","example_input_batch.shape, example_target_batch.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d9L9hFHTyWeK","executionInfo":{"status":"ok","timestamp":1652392619793,"user_tz":240,"elapsed":7,"user":{"displayName":"Ala' Hashesh","userId":"02371999640038913792"}},"outputId":"f2f072b9-891e-4353-9bc3-afab8643e4d9"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([32, 103]), TensorShape([32, 123]))"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# Encoder class\n","class Encoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n","    super(Encoder, self).__init__()\n","    self.batch_sz = batch_sz\n","    self.enc_units = enc_units\n","\n","    # Embed the vocab to a dense embedding \n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","\n","    # GRU Layer\n","    # glorot_uniform: Initializer for the recurrent_kernel weights matrix, \n","    # used for the linear transformation of the recurrent state\n","    self.gru = tf.keras.layers.GRU(self.enc_units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","\n","  # Encoder network comprises an Embedding layer followed by a GRU layer\n","  def call(self, x, hidden):\n","    x = self.embedding(x)\n","    output, state = self.gru(x, initial_state=hidden)\n","    return output, state\n","\n","  # To initialize the hidden state\n","  def initialize_hidden_state(self):\n","    return tf.zeros((self.batch_sz, self.enc_units))"],"metadata":{"id":"Je1BSTmuyZuS","executionInfo":{"status":"ok","timestamp":1652392619794,"user_tz":240,"elapsed":7,"user":{"displayName":"Ala' Hashesh","userId":"02371999640038913792"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n","\n","sample_hidden = encoder.initialize_hidden_state()\n","sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n","\n","print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n","print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L42ElNwOydiU","executionInfo":{"status":"ok","timestamp":1652392620710,"user_tz":240,"elapsed":922,"user":{"displayName":"Ala' Hashesh","userId":"02371999640038913792"}},"outputId":"8ae47127-0b62-43a2-da9e-53f13f739cbd"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoder output shape: (batch size, sequence length, units) (32, 103, 1024)\n","Encoder Hidden state shape: (batch size, units) (32, 1024)\n"]}]},{"cell_type":"code","source":["# Attention Mechanism\n","class BahdanauAttention(tf.keras.layers.Layer):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, query, values):\n","    # query hidden state shape == (batch_size, hidden size)\n","    # values shape == (batch_size, max_len, hidden size)\n","\n","    # we are doing this to broadcast addition along the time axis to calculate the score\n","    # query_with_time_axis shape == (batch_size, 1, hidden size)\n","    query_with_time_axis = tf.expand_dims(query, 1)\n","\n","    # score shape == (batch_size, max_length, 1)\n","    # we get 1 at the last axis because we are applying score to self.V\n","    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n","    score = self.V(tf.nn.tanh(\n","        self.W1(query_with_time_axis) + self.W2(values)))\n","\n","    # attention_weights shape == (batch_size, max_length, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","\n","    # context_vector shape after sum == (batch_size, hidden_size)\n","    context_vector = attention_weights * values\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector, attention_weights"],"metadata":{"id":"A6LZdIsjygug","executionInfo":{"status":"ok","timestamp":1652392620710,"user_tz":240,"elapsed":12,"user":{"displayName":"Ala' Hashesh","userId":"02371999640038913792"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["attention_layer = BahdanauAttention(10)\n","attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n","\n","print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n","print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FcDKWxqSykJe","executionInfo":{"status":"ok","timestamp":1652392620711,"user_tz":240,"elapsed":13,"user":{"displayName":"Ala' Hashesh","userId":"02371999640038913792"}},"outputId":"433f2038-bc48-4dc5-e96f-f65a8298b03a"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Attention result shape: (batch size, units) (32, 1024)\n","Attention weights shape: (batch_size, sequence_length, 1) (32, 103, 1)\n"]}]},{"cell_type":"code","source":["# Decoder class\n","class Decoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n","    super(Decoder, self).__init__()\n","    self.batch_sz = batch_sz\n","    self.dec_units = dec_units\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(self.dec_units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    self.fc = tf.keras.layers.Dense(vocab_size)\n","\n","    # Used for attention\n","    self.attention = BahdanauAttention(self.dec_units)\n","\n","  def call(self, x, hidden, enc_output):\n","    # x shape == (batch_size, 1)\n","    # hidden shape == (batch_size, max_length)\n","    # enc_output shape == (batch_size, max_length, hidden_size)\n","\n","    # context_vector shape == (batch_size, hidden_size)\n","    # attention_weights shape == (batch_size, max_length, 1)\n","    context_vector, attention_weights = self.attention(hidden, enc_output)\n","\n","    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","    x = self.embedding(x)\n","\n","    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","    # passing the concatenated vector to the GRU\n","    output, state = self.gru(x)\n","\n","    # output shape == (batch_size * 1, hidden_size)\n","    output = tf.reshape(output, (-1, output.shape[2]))\n","\n","    # output shape == (batch_size, vocab)\n","    x = self.fc(output)\n","\n","    return x, state, attention_weights"],"metadata":{"id":"tVpdm40nymQi","executionInfo":{"status":"ok","timestamp":1652392620711,"user_tz":240,"elapsed":10,"user":{"displayName":"Ala' Hashesh","userId":"02371999640038913792"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n","\n","sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n","                                      sample_hidden, sample_output)\n","\n","print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y4SYJkeuyoez","executionInfo":{"status":"ok","timestamp":1652392620712,"user_tz":240,"elapsed":10,"user":{"displayName":"Ala' Hashesh","userId":"02371999640038913792"}},"outputId":"0762c5df-7120-45f7-c59e-96155ea06b16"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Decoder output shape: (batch_size, vocab size) (32, 12397)\n"]}]},{"cell_type":"code","source":["# Initialize optimizer and loss functions\n","optimizer = tf.keras.optimizers.Adam()\n","\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","# Loss function\n","def loss_function(real, pred):\n","\n","  # Take care of the padding. Not all sequences are of equal length.\n","  # If there's a '0' in the sequence, the loss is being nullified\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  return tf.reduce_mean(loss_)"],"metadata":{"id":"6i3RxRgjyrN0","executionInfo":{"status":"ok","timestamp":1652392620713,"user_tz":240,"elapsed":9,"user":{"displayName":"Ala' Hashesh","userId":"02371999640038913792"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n","                                 encoder=encoder,\n","                                 decoder=decoder)"],"metadata":{"id":"kpT98OrPyuQe","executionInfo":{"status":"ok","timestamp":1652392620713,"user_tz":240,"elapsed":8,"user":{"displayName":"Ala' Hashesh","userId":"02371999640038913792"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["@tf.function\n","def train_step(inp, targ, enc_hidden):\n","  loss = 0\n","\n","  # tf.GradientTape() -- record operations for automatic differentiation\n","  with tf.GradientTape() as tape:\n","    enc_output, enc_hidden = encoder(inp, enc_hidden)\n","\n","    # dec_hidden is used by attention, hence is the same enc_hidden\n","    dec_hidden = enc_hidden\n","\n","    # <start> token is the initial decoder input\n","    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n","\n","    # Teacher forcing - feeding the target as the next input\n","    for t in range(1, targ.shape[1]):\n","\n","      # Pass enc_output to the decoder\n","      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n","\n","      # Compute the loss\n","      loss += loss_function(targ[:, t], predictions)\n","\n","      # Use teacher forcing\n","      dec_input = tf.expand_dims(targ[:, t], 1)\n","\n","  # As this function is called per batch, compute the batch_loss\n","  batch_loss = (loss / int(targ.shape[1]))\n","\n","  # Get the model's variables\n","  variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","  # Compute the gradients\n","  gradients = tape.gradient(loss, variables)\n","\n","  # Update the variables of the model/network\n","  optimizer.apply_gradients(zip(gradients, variables))\n","\n","  return batch_loss"],"metadata":{"id":"rjxEUQv4y2e5","executionInfo":{"status":"ok","timestamp":1652392620714,"user_tz":240,"elapsed":8,"user":{"displayName":"Ala' Hashesh","userId":"02371999640038913792"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["import time\n","\n","EPOCHS = 5\n","\n","# Training loop\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  # Initialize the hidden state\n","  enc_hidden = encoder.initialize_hidden_state()\n","  total_loss = 0\n","\n","  # Loop through the dataset\n","  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n","\n","    # Call the train method\n","    batch_loss = train_step(inp, targ, enc_hidden)\n","\n","    # Compute the loss (per batch)\n","    total_loss += batch_loss\n","\n","    if batch % 100 == 0:\n","      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n","                                                   batch,\n","                                                   batch_loss.numpy()))\n","  # Save (checkpoint) the model every 2 epochs\n","  if (epoch + 1) % 2 == 0:\n","    checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","  # Output the loss observed until that epoch\n","  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n","                                      total_loss / steps_per_epoch))\n","  \n","  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tx-M1TNqy3lK","outputId":"4f27a63b-1dee-4e12-ebc4-f7cf7a337e20"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1 Batch 0 Loss 0.7167\n","Epoch 1 Batch 100 Loss 0.4785\n","Epoch 1 Batch 200 Loss 0.6560\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Baseline.ipynb","provenance":[{"file_id":"1cZRsCkGlDEJy1L5Rs80tr5eNonzwXVn0","timestamp":1643318874030},{"file_id":"1wgo33YMqyTmwPXBCgDYvDD39Hgz516zV","timestamp":1630356584355},{"file_id":"1ZNQQshRjVp-0vLNi6ZGRtXX102EWHRq4","timestamp":1598302241860},{"file_id":"1XOa--UHuAQpBRcdqYbFcb8QuUTvywsSk","timestamp":1568522504552},{"file_id":"1LShMg_-e2SzrjDMxSgVbnYyTAgwcJov0","timestamp":1568420694683}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}